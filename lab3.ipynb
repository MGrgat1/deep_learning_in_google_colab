{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjWrsG0jA+Vts/tAdfUTuT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "RPeBhQW16pdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x - the domain over which the function is evaluated\n",
        "\n",
        "y_expected - the expected values of the function\n",
        "\n",
        "number_of_iterations - how many times we'll run the training\n",
        "\n",
        "training_graph_display_rate - after how many times we'll show the graph\n"
      ],
      "metadata": {
        "id": "37zU5zQK5i_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate):\n",
        "\n",
        "  # set up the neural network that will approximate the function\n",
        "\n",
        "  # Every x in the polynomial (x, x^2, x^3) is its own function\n",
        "  # unsqueeze maps from (2000) to (2000, 1)\n",
        "  # pow uses all x parameters as powers for the tensor p\n",
        "  p = torch.tensor([1, 2, 3, 4, 5], device=device)\n",
        "  xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "  # a simple linear sequential model\n",
        "  # input - 3, output - 1\n",
        "  # Flatten maps the output to a 1D vector\n",
        "  model = torch.nn.Sequential(\n",
        "      torch.nn.Linear(5, 1),\n",
        "      torch.nn.Flatten(0, 1)\n",
        "  ).cuda()\n",
        "\n",
        "  # mean square error with error summation\n",
        "  loss_fn = torch.nn.MSELoss(reduction='sum').cuda()\n",
        "\n",
        "  #SGD optimizer\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  #set up loss statistics\n",
        "  t_samples = []\n",
        "  loss_samples = []\n",
        "  loss_gradient_samples = []\n",
        "  previous_loss = 0\n",
        "  current_loss = 0\n",
        "  loss_delta = 0\n",
        "\n",
        "\n",
        "\n",
        "  # the training loop\n",
        "  print(\"Entering training:\")\n",
        "  for t in range(number_of_iterations):\n",
        "      # forward propagation\n",
        "      # y = a + b x + c x^2 + d x^3\n",
        "      y_pred = model(xx)\n",
        "\n",
        "      # calculate the error using a built-in error function\n",
        "      loss = loss_fn(y_pred, y_expected)\n",
        "\n",
        "      # determine the rate at which the losses are changing\n",
        "      current_loss = loss.item()\n",
        "      loss_delta = previous_loss - current_loss\n",
        "      previous_loss = current_loss\n",
        "\n",
        "      # reset the gradient using the optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # backpropagation\n",
        "      # calculates the gradient with respect to all three parameters of the model\n",
        "      # all parameters within the model are tensors with the property requires_grad=True\n",
        "      loss.backward()\n",
        "\n",
        "      # update the parameters using the optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # get the parameters\n",
        "      linear_layer = model[0]\n",
        "\n",
        "      a = linear_layer.bias.item()\n",
        "      b = linear_layer.weight[:, 0].item()\n",
        "      c = linear_layer.weight[:, 1].item()\n",
        "      d = linear_layer.weight[:, 2].item()\n",
        "      e = linear_layer.weight[:, 3].item()\n",
        "      f = linear_layer.weight[:, 4].item()\n",
        "\n",
        "      # display the learning process\n",
        "      if t % training_graph_display_rate == 0:\n",
        "\n",
        "          # gather the loss samples\n",
        "          t_samples.append(t)\n",
        "          loss_samples.append(current_loss)\n",
        "          loss_gradient_samples.append(loss_delta)\n",
        "          print(f'Iteration: {t}, loss:  {current_loss}, loss_delta: {loss_delta}')\n",
        "\n",
        "          # plot the functions\n",
        "          y_graph = a + b * x + c * x ** 2 + d * x ** 3 + e * x ** 4 + f * x ** 5\n",
        "          plt.plot(x.cpu().detach(), y_expected.cpu().detach(), '-r', label='expected function', color = \"red\") \n",
        "          plt.plot(x.cpu().detach(), y_graph.cpu().detach(), '-r', label='trained function', color =\"blue\")\n",
        "          plt.title('Approximation')\n",
        "          plt.xlabel('x', color='#1C2833')\n",
        "          plt.ylabel('y', color='#1C2833')\n",
        "          plt.legend(loc='upper left')\n",
        "          plt.grid()\n",
        "          plt.show()\n",
        "\n",
        "  print(f\"Training finished after {t} iterations\")\n",
        "  print(f\"x_min: {x_min}\")\n",
        "  print(f\"x_max: {x_max}\")\n",
        "  print(f\"Learning rate: {learning_rate},  \")\n",
        "  print(f\"Current loss:{current_loss}\")\n",
        "  print(f\"Loss delta:{loss_delta}\")\n",
        "\n",
        "  print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3 + {e} x^4 + {f} x^5')\n",
        "\n",
        "  # a graph of the losses\n",
        "  plt.plot(t_samples, loss_samples, '-r', label='losses')\n",
        "  plt.title('Losses')\n",
        "  plt.xlabel('Number of iterations', color='#1C2833')\n",
        "  plt.ylabel('Loss', color='#1C2833')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.grid()\n",
        "  print(\"Losses:\")\n",
        "  plt.show()\n",
        "\n",
        "  # a graph of the losses\n",
        "  plt.plot(t_samples, loss_gradient_samples, '-r', label='loss gradients', color=\"#232C33\")\n",
        "  plt.title('Losses')\n",
        "  plt.xlabel('Number of iterations', color='#1C2833')\n",
        "  plt.ylabel('Loss', color='#1C2833')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.grid()\n",
        "  print(\"Loss gradients:\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mKpSn_UIRpNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximating the function:\n",
        "y=exp(x)\n",
        "\n",
        "Examples of intervals (x_min, x_max) on which the function converges:\n",
        "\n",
        "(-0.75,0.25)\n",
        "(-0.5,0.5)\n",
        "(1,e)\n",
        "(e-1,e)\n",
        "\n",
        "For intervals where x_min < 4 or x_max > e the function fails to converge (exploding gradient problem)\n",
        "\n",
        "Examples:\n",
        "(-5,-3)\n",
        "(e,2e)\n"
      ],
      "metadata": {
        "id": "O4jFZooOwZVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 2000\n",
        "number_of_iterations = 100000\n",
        "interval_length = math.pi\n",
        "x_min = -math.pi/2\n",
        "x_max = x_min + interval_length\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = torch.exp(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate)"
      ],
      "metadata": {
        "id": "jH97ZSG6XUbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximating the function:\n",
        "y=x*sin(x)\n",
        "\n",
        "Examples of intervals (x_min, x_max) on which the function converges:\n",
        "\n",
        "(pi,2pi)\n",
        "(0,pi)\n",
        "(-pi/2, pi/2)\n",
        "(-pi, 0)\n",
        "\n",
        "\n",
        "For intervals where x_min < -pi or x_max > 2pi the function fails to converge (exploding gradient problem)\n",
        "\n",
        "Examples:\n",
        "(-2pi,-pi)\n",
        "(2pi,3pi)"
      ],
      "metadata": {
        "id": "e0E1VllGwT_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 2000\n",
        "number_of_iterations = 100000\n",
        "interval_length = math.pi\n",
        "x_min = -2*math.pi\n",
        "x_max = x_min + interval_length\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = x*torch.sin(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate)"
      ],
      "metadata": {
        "id": "pZAn6xDSdDnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximating the function:\n",
        "y=cos(x)*sin(x)\n",
        "\n",
        "Examples of intervals (x_min, x_max) on which the function converges:\n",
        "(-pi/2,pi/2)\n"
      ],
      "metadata": {
        "id": "mxbJvd_dwO_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 2000\n",
        "number_of_iterations = 100000\n",
        "interval_length = math.pi\n",
        "x_min = -math.pi/2\n",
        "x_max = x_min + interval_length\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = torch.cos(x)*torch.sin(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate)"
      ],
      "metadata": {
        "id": "kaPHfYvnf-Lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}