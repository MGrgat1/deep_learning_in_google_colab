{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtHG2SR7t1b5KiBZEEmx5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MGrgat1/deep_learning_in_google_colab/blob/main/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "metadata": {
        "id": "2Sm9DfqWRUEx"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate, stagnation_tolerance_rate):\n",
        "\n",
        "  # set up the neural network that will approximate the function\n",
        "\n",
        "  # Every x in the polynomial (x, x^2, x^3) is its own function\n",
        "  # unsqueeze maps from (2000) to (2000, 1)\n",
        "  # pow uses all x parameters as powers for the tensor p\n",
        "  p = torch.tensor([1, 2, 3, 4, 5], device=device)\n",
        "  xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "  # a simple linear sequential model\n",
        "  # input - 3, output - 1\n",
        "  # Flatten maps the output to a 1D vector\n",
        "  model = torch.nn.Sequential(\n",
        "      torch.nn.Linear(5, 1),\n",
        "      torch.nn.Flatten(0, 1)\n",
        "  ).cuda()\n",
        "\n",
        "  # mean square error with error summation\n",
        "  loss_fn = torch.nn.MSELoss(reduction='sum').cuda()\n",
        "\n",
        "  #SGD optimizer\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  #set up loss statistics\n",
        "  t_samples = []\n",
        "  loss_samples = []\n",
        "  loss_gradient_samples = []\n",
        "  previous_loss = 0\n",
        "  current_loss = 0\n",
        "  loss_delta = 0\n",
        "  stagnation_counter = 0\n",
        "  total_stagnation = 0\n",
        "  accepted_number_of_stagnation = 500\n",
        "\n",
        "\n",
        "\n",
        "  # the training loop\n",
        "  print(\"Entering training:\")\n",
        "  for t in range(number_of_iterations):\n",
        "      # forward propagation\n",
        "      # y = a + b x + c x^2 + d x^3\n",
        "      y_pred = model(xx)\n",
        "\n",
        "      # calculate the error using a built-in error function\n",
        "      loss = loss_fn(y_pred, y_expected)\n",
        "\n",
        "      # determine the rate at which the losses are changing\n",
        "      current_loss = loss.item()\n",
        "      loss_delta = previous_loss - current_loss\n",
        "      previous_loss = current_loss\n",
        "\n",
        "\n",
        "      # if the rate of change of losses is below a certain level (stagnation tolerance rate), we've probably reached the local minimum\n",
        "      if abs(loss_delta) < stagnation_tolerance_rate:\n",
        "        stagnation_counter += 1\n",
        "      else:\n",
        "        total_stagnation += stagnation_counter\n",
        "        stagnation_counter = 0\n",
        "\n",
        "      if stagnation_counter > accepted_number_of_stagnation:\n",
        "        print(f\"Training has reached the local minimum\")\n",
        "        break\n",
        "\n",
        "      # reset the gradient using the optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # backpropagation\n",
        "      # calculates the gradient with respect to all three parameters of the model\n",
        "      # all parameters within the model are tensors with the property requires_grad=True\n",
        "      loss.backward()\n",
        "\n",
        "      # update the parameters using the optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # get the parameters\n",
        "      linear_layer = model[0]\n",
        "\n",
        "      a = linear_layer.bias.item()\n",
        "      b = linear_layer.weight[:, 0].item()\n",
        "      c = linear_layer.weight[:, 1].item()\n",
        "      d = linear_layer.weight[:, 2].item()\n",
        "      e = linear_layer.weight[:, 3].item()\n",
        "      f = linear_layer.weight[:, 4].item()\n",
        "\n",
        "      # display the learning process\n",
        "      if t % training_graph_display_rate == 0:\n",
        "\n",
        "          # gather the loss samples\n",
        "          t_samples.append(t)\n",
        "          loss_samples.append(current_loss)\n",
        "          loss_gradient_samples.append(loss_delta)\n",
        "          print(f'Iteration: {t}, loss:  {current_loss}, loss_delta: {loss_delta}')\n",
        "\n",
        "          # plot the functions\n",
        "          y_graph = a + b * x + c * x ** 2 + d * x ** 3 + e * x ** 4 + f * x ** 5\n",
        "          plt.plot(x.cpu().detach(), y_expected.cpu().detach(), '-r', label='expected function', color = \"red\") \n",
        "          plt.plot(x.cpu().detach(), y_graph.cpu().detach(), '-r', label='trained function', color =\"blue\")\n",
        "          plt.title('Approximation')\n",
        "          plt.xlabel('x', color='#1C2833')\n",
        "          plt.ylabel('y', color='#1C2833')\n",
        "          plt.legend(loc='upper left')\n",
        "          plt.grid()\n",
        "          plt.show()\n",
        "\n",
        "  print(f\"Training finished after {t} iterations\")\n",
        "  print(f\"x_min: {x_min}\")\n",
        "  print(f\"x_max: {x_max}\")\n",
        "  print(f\"Learning rate: {learning_rate},  \")\n",
        "  print(f\"Stagnation tolerance rate: {stagnation_tolerance_rate},  \")\n",
        "  print(f\"Current loss:{current_loss}\")\n",
        "  print(f\"Loss delta:{loss_delta}\")\n",
        "  print(f\"Total stagnation:{total_stagnation}\")\n",
        "\n",
        "  print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3 + {e} x^4 + {f} x^5')\n",
        "\n",
        "  # a graph of the losses\n",
        "  plt.plot(t_samples, loss_samples, '-r', label='losses')\n",
        "  plt.title('Losses')\n",
        "  plt.xlabel('Number of iterations', color='#1C2833')\n",
        "  plt.ylabel('Loss', color='#1C2833')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.grid()\n",
        "  print(\"Losses:\")\n",
        "  plt.show()\n",
        "\n",
        "  # a graph of the losses\n",
        "  plt.plot(t_samples, loss_gradient_samples, '-r', label='loss gradients', color=\"#232C33\")\n",
        "  plt.title('Losses')\n",
        "  plt.xlabel('Number of iterations', color='#1C2833')\n",
        "  plt.ylabel('Loss', color='#1C2833')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.grid()\n",
        "  print(\"Loss gradients:\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mKpSn_UIRpNa"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 1000\n",
        "number_of_iterations = 100000\n",
        "x_min = -math.pi\n",
        "x_max = math.pi\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = torch.sin(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate)"
      ],
      "metadata": {
        "id": "gPm4IElMEab4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 2000\n",
        "stagnation_tolerance_rate = 0.00001\n",
        "number_of_iterations = 100000\n",
        "interval_length = 1\n",
        "x_min = -4\n",
        "x_max = x_min + interval_length\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = torch.exp(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate, stagnation_tolerance_rate)"
      ],
      "metadata": {
        "id": "jH97ZSG6XUbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 2000\n",
        "stagnation_tolerance_rate = 0.00001\n",
        "number_of_iterations = 100000\n",
        "interval_length = math.pi\n",
        "x_min = 1\n",
        "x_max = x_min + interval_length\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = x*torch.sin(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate, stagnation_tolerance_rate)"
      ],
      "metadata": {
        "id": "pZAn6xDSdDnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_graph_display_rate = 2000\n",
        "stagnation_tolerance_rate = 0.00001\n",
        "number_of_iterations = 100000\n",
        "interval_length = math.pi\n",
        "x_min = -math.pi/2\n",
        "x_max = x_min + interval_length\n",
        "x = torch.linspace(x_min, x_max, 2000, device=device, dtype=dtype)\n",
        "y_expected = torch.cos(x)*torch.sin(x)\n",
        "learning_rate = 1e-6\n",
        "perform_approximation(x, y_expected, learning_rate, number_of_iterations, training_graph_display_rate, stagnation_tolerance_rate)"
      ],
      "metadata": {
        "id": "kaPHfYvnf-Lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}